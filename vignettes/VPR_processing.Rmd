---
title: "VPR_processing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{VPR_processing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

author: Emily Chisholm
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(vprr)
```


#Introduction

This document was produced at Bedford Institute of Oceanogrpahy (BIO) to accompany the vprr package, a data analysis and visualization package for the Visual Plankton Recorder (VPR). It will summarize the characteristics of the VPR instrument and it's usage, as well as the processing procedure followed at BIO and the data products produced. It will also describe visualization techniques and give an overview of file types and directory structures. A glossary section is included at the end of this document.

The scope of this document is focused on the portion of data analysis done in R. Before data is cleaned and visulaized in R it goes through initial image processing software (autoDeck) and machine learning classification software in MATLAB (Visual Plankton).

#The Instrument

The Visual Plankton Recorder (VPR) was initially designed at Woods Hole Oceanographic Institute (WHOI), where Cabell Davis was a key contributor. It is an oceanogrpahic instrument which combines the properties of a CTD and high quality underwater photography with plankton nets and microscopy. The VPR itself contains two distinct instruments which log data simultaneously, the CTD and the camera. These two instruments work together to give a detailed picture of the microscopic plankton community in the ocean. 

The CTD which is connected to the VPR is a seaBird product and can accomodate many of the sensors typically found on a CTD which would be deployed with a Rosette. At BIO we have typically included fluorescence and turbidity sensors to accompany standard depth, temperature and salinity measurements. This CTD is consistently logging data throughout the VPR cast. 

The camera which is attached to the VPR is capturing approximately 15 frames per second of a very small volume of water directly in front of the instrument. The volume of water captured in the frame depends on the optical setting of the VPR, which can be S0, S1, S2 or S3, where S0 has the greatest magnification and smallest image volume, and S3 has the least magnification and largest image volume. The types of organisms captured depends on the magnification and image volume to some extent and has been optimized at BIO to target the size range of our species of interest.

The output of the instrument is a series of image files as well as CTD data files. 

The instrument can be deployed in either a tow yo or profile pattern. The instrument at BIO has been optimized for tow yo deployments with a large  fin which encourages the instrument to glide smoothly through the water column. This document will not cover profile deployment data.

#Summary of Processing Steps

The processing of VPR data in R can be broken down into 4 steps. 

1. AutoID Copying - Organizing VPR images into appropriate folders
2. Reclassification - A manual check of Visual Plankton's classifications which allows expert corrections
3. Processing - Formatting raw VPR data and images into dataframes which are quality controlled and can then be exported, analyzed and visualized
4. Plotting - Visualization of processed data frames using ggplot2

Before beginning processing, it is reccommended to create a VPR processing environment which will contain commonly used variables and file paths. The simplest and most reproducible way to achieve this is to write an R script where all your cruise and system specific varibales are contained, then save the environement as an RData file to be loaded in at the start of any processing scripts. 

```
#set VPR processing environment

#SET WORKING DIRECTORY
wd <- "C:/VPR_PROJECT/COR2019002/SCRIPTS"
setwd(wd)

#CRUISE
cruise <- 'COR2019002'

#CSV FILE WITH STATION NAMES AND CORRESPONDING DAY/HOUR INFO
station_names_file <- paste0("station_names_", cruise, ".csv")
#example: 'C:/VPR_PROJECT/vp_info/station_names_COR2019002.csv'

#DIRECTORY FOR FULL CTD CAST DATA
castdir <- paste0('D:/', cruise, "/", cruise, "_autodeck/")
#example: 'D:/COR2019002/COR2019002_autodeck/'


#AUTOID FOLDER FOR MEASUREMENT DATA
drive <- 'C:/'
auto_id_folder <- paste0(drive, "cruise_", cruise, "/", autoid)
#example: 'E:/COR2019002/autoid' #!!NO BACKSLASH AT END OF STRING

#Path to autoids for each taxa
auto_id_path <- list.files(paste0(auto_id_folder, "/"), full.names = T) #caution if there are autoid versions

#create standard directory for saved data files per cruise
savedir <- paste0(cruise, '_data_files')
dir.create(savedir, showWarnings = FALSE)

#create standard directory for saved data products per cruise and station
stdir <- paste('data_product/', cruise, sep = "")
dir.create(stdir, showWarnings = FALSE, recursive = TRUE)

#DEPTH BIN SIZE FOR AVERAGING
binSize <- 3

####SAVE####
#SAVE ALL FILE PATHS AND SETTINGS AS PROJECT ENVIRONMENT

save.image(file = paste0(cruise,'_env.RData'))
```

Once this environment is set, it can be loaded into any processing session by using
```
load('COR2019002_env.RData')
```
These variable names will be kept consistent throughout this document for simplicity.

These variables could also be set at the start of each processing script, or manually as you run through processing but containing them all within an environnment allows for neater code. It also means file paths and specifics are easier to change if needed, for example if processing a new cruise. If sharing your processing code with colleagues on version control, keeping your environment variables seperate (outside of your git project) will allow you to collaborate without breaking each other's code due to difference in file paths or folder names. 


#1. AutoID Copying

The first step in processing VPR data is to organize the images in relevant folders, based on the day and hour of data collection as well as the classification of the organism in the image, as decided by the machine learning software in Visual Plankton (VP). The images are originally organized by day and hour but reorganizing them based on classification allows easier human interaction with the images and data. Directory structure will be considered in more depth in a later section.

To properly organize the images for a given cruise, use the function `vprr::autoid_copy()`. This function will automatically organize your VPR image files into folders based on day and hour of data collection as well as classification value of the image from Visual Plankton. This structure is used by the next step of processing in order to confirm Visual Plankton's classifications.

```
# create variables
# ---------------------
basepath <- "C:\\data\\cruise_COR2019002\\autoid\\" 
# note this is the same as your auto_id_folder environment variable excpet the file seperator is different, 
# because this script will run source code in command line which does not recognize '/' as a file seperator
day <- "123"
hour <- "01" # note leading zero is kept
classifier_type <- "svm"
classifier_name <- "myclassifier"

# run file organizer
# ---------------------
autoid_copy(basepath, day, hour, classifer_type, classifier_name)
```

Once image files are organized you can easily look through classification groups manually (in file explorer) to get a sense of which groups are well classified by Visual Plankton and which could benefit from some additional sorting - Step 2.

#2. Reclassification

This step is optional although it is reccommended by folks at BIO have processed this data. 

The goal of this step is to improve upon the automatic classifications produced by Visual Plankton (VP). The supervised machine learning in VP is a great tool for sorting VPR images but cannot capture some of the finer resolution desired by oceanographers concerned with plankton community dynamics. For example, the VP software is great at distinguishing marine snow from copepods but not good at distinguishing between species of copepods. The level of detail desired for most projects is in line with an expert taxonomic analysis, so further sorting is required after VP. The key to this step being successful is a patient taxonomic expert who is able to properly identify organisms. 

There are a series of functions written to achieve this goal. VPR images are displayed on the screen one at a time for analysis and classification, if VP has misclassified an image or if it can be recognized as falling into a finer resolution category the image can be reclassified. At the end of this process text files which match the formats produced by VP are reproduced with new classifications. 

The reclassification process is reccommended to be run for only specific taxa which VP has clearly struggled to classify based on manual inspection of image folders. This will also depend on the groups of interest to researchers. In analysis performed by BIO, Calanus and krill were the main species of interest. VP frequently misclassified krill images in the chaetognath and ctenophore categories, so reclassification experts went through the calanus, krill, chaetognath and ctenophore image folders. It is ideal to not have to reclassify all image folders to improve efficiency, at the very least, we would reccommend not sorting through marine snow or blurry image folders, as VP does a good job of classifying these images and the folders can be extremely large and time consuming to sort through. 

Reclassification can also be an opportunity to create new, fine resolution classification groups which could not previously be identified by Visual Plankton. For example, if Visual Plankton identifies a classification group 'copepods' but it would be helpful to split this group into small and large copepods for analysis, small and large copepod classification groups can be added during reclassification. By adding new classification groups to your 'taxa of interest' (`taxa_of_interest` variable in example below), it allows the expert performing reclassification to label images with these new classification groups. In the example below, Metridia and Paraeuchaeta were added as new classification groups that allowed finer resolution zooplankton community data. After reclassification is run, new files are created for new taxa which are identical in format to original files produced by VP. This feature has become very useful for getting finer scale species data, it would also be useful if data was being reprocessed with a new focal species or group of interest because it would allow an expert to dig deeper into a specific classification group. It can also be useful if, during expert reclassification, a certain classification group which was not included in VP classifications, is noticed to be prominent.


There are three sections of this reclassification. 

Step A includes prepping the environment by setting some variables. The processing environment is loaded, which includes the `auto_id_folder` variable. Day and hour of interest should be set manually. Most experts would agree that an hour of data represents a significant chunk of work and you will likely want to take breaks in between reclassifying hours. Your taxa of interest should also be set, these taxa are the existing VP classification groups you will be sorting through, as well as the new taxa you may want to reclassify images under. The `vprr::add_new_taxa()` function sets up the folder structure for any new taxa which have been added to your list of interest. Then the reclassification is run with `vprr::clf_check()`. `vprr::clf_check()` has a few optional arguments to customize the reclassification experience, notably `gr` which is a logical value determining whether or not reclassification options appear as pop ups or in the ocmmand line, as well as `img_bright` which is a logical which determines whether or not the original image is appended with an extra bright version of the image. Having a bright version of the image allows experts to see the outline of the organism better, any thin appendages become more clear and gelatinous organisms like chaetognaths or ctenophores are easier to distinguish. 


```
#### STEP 2- A : CLASSIFICATION CHECK
# -------------------------------------

# Once classified images are sorted by taxa
# ensure classification accuracy by manually 
# looking through classified images

#### USER INPUT REQUIRED ####     

load('COR2019002_env.RData')

day <- '235' 
hr <- '19' # keep leading zeros, must be two characters


taxa_of_interest <-
  c(
    'krill',
    'Calanus',
    'chaetognaths',
    'ctenophores',
    'Other',
    'larval_fish',
    'marine_snow',
    'small_copepod',
    'other_copepods',
    'larval_crab',
    'amphipod',
    'Metridia',
    'Paraeuchaeta',
    'cnidarians'
    
  )


# add new taxa (optional)

add_new_taxa(taxa = taxa_of_interest, auto_id_folder)
# ensures there is proper folder structure for all taxa of interest

# reclassify images
clf_check(day = day, hour= hr, basepath = auto_id_folder,gr = FALSE, 
          taxa_of_interest = taxa_of_interest, scale = 'x300',
          opticalSetting = 'S3')
```

Step B should be run after each hour of data is completed although it can be run at any point following the reclassification of an hour of data. `vprr::clf_check()` produces two file types as a record of reclassification, this step takes those output files and uses them to appropriately modify the original VP files. These output files are found in your working directory, orgnized inside folders named by the day and hour that the data was collected. The function `vprr::new_aids()` takes these reclassification files and outputs VP format classification files, into your auto id folder directory. 

```

#### STEP 2 - B: REORGANIZE ROIS 
# -----------------------------------------

day_hour_files <-  paste0('d', day, '.h', hr)

misclassified <- list.files(day_hour_files, pattern = 'misclassified_', full.names = TRUE)

reclassify <- list.files(day_hour_files, pattern = 'reclassify_', full.names = TRUE)


# MOVE ROIS THAT WERE MISCLASSIFIED INTO CORRECT FILES
new_aids(reclassify, misclassified, auto_id_folder)


```

Step C, the last step of reclassifcation includes some manual file organization and final checks. Once all classification (aid & aidmeas) files have been created, they are contained in taxa folders inside your working directory. These files need to be reorganized within your original Visual Plankton directory. You should use the new aid and aidmeas files created to replace the original VP files where you have performed reclassification. Remember this may not be for all classification groups, or all days/hours of data. You may also want to archive a copy of the orginal classification files, in case you need to rerun processing, this will prevent you from having to rerun Visual Plankton in MATLAB which can be time consusming. Once files have been manually organized, there are two checks that can be performed. `vprr::aid_file_check()` is strongly recommended, it removes any empty aid files which can cause errors in processing down the line. It also checks that aid and aidmeas files are matching within an hour of data, that they include the same number of ROIs, and that the VPR tow number for all files is the same. Empty files are created if there are no images of a specific classification group in that hour of data, but should be removed as they do not contain any data. There have been bugs in the past where aid and aidmeas files within an hour were different lengths, because these files represent the same data but the only key between them is line number, it is important that they represent the same number of data points. There was also a bug in the past where some images from different VPR tows were getting pulled together due to a formatting error in raw data on IML2018051. 


```

#ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT #
#==========================================================================================#
#                            ONCE ALL AID FILES ARE COMPLETE                               #
#==========================================================================================#
#                                     REQUIRED!!!!                                         #
#       manual organization of new aid and aidmeas files into base path directory          #
#==========================================================================================#


#### STEP 2 - C: CHECK FILES
# --------------------------------


# aid check step
# removes empty aid files, and checks for errors in writing

aid_file_check(basepath, cruise) #OUTPUT: text log 'CRUISE_aid_file_check.txt in working directory


# optional -----------------------------------------------------------------------------------------------------------------------------------
# manual krill check can be done to verfiy krill images. 
# Sometimes this is required because of the low volume of krill, means high sensitivity to error.
# Use: krill_check2.R


```



