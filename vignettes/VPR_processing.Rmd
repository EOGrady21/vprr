---
title: "VPR_processing"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{VPR_processing}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}

author: Emily Chisholm, Kevin Sorochan
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(vprr)
```


# Introduction

This document was produced at Bedford Institute of Oceanography (BIO) to accompany the vprr package, a data analysis and visualization package for the Video Plankton Recorder (VPR). It will summarize processing procedure followed at BIO and the data products produced. 


The Video Plankton Recorder (VPR) was initially designed at Woods Hole Oceanographic Institute (WHOI), it is a SeaScan product. There are many versions of the VPR, this document refers to the DAVPR model simply as VPR. It is an oceanographic instrument which combines a CPU, camera and CTD (SBE49). The CPU and camera work together to capture microscopic images of the plankton community. The output of the instrument is a series of image files as well as CTD data files. The instrument can be deployed in either a tow yo or profile pattern, this document focuses on tow-yo deployments. Further details on the instrument can be found (***SEASCAN MANUAL LINK***). 


The VPR outputs two data types, images as well as CTD text files. These two file types are linked by time. Raw VPR data goes through three processing steps, first it is run through a SeaScan software (Autodeck) which is supplied with the instrument. Next, the processed images are classified using Visual Plankton. The original Visual Plankton can be obtained here (***WHOI LINK***), and a modified version can be found here (***VP GITHUB LINK***). The modified version has been updated by researchers at BIO, most notably it uses an updated machine learning algorithm. VP outputs specific text files which describe the classification and size of individual regions of interest (ROIs). The files from VP which are used to complete processing in R include the autoid files, organized by taxa into aid and aidmeas files. Aid files represent file paths to individual ROI images and can be used to organize classifed groups, aidmeas files contain size data for each ROI within a group. 

**flowchart?**

Before data is cleaned and visualized in R it goes through initial image processing software (autoDeck) and machine learning classification software in MATLAB (Visual Plankton). The goal of this section of VPR processing is to combine the VPR CTD output with the classified ROI data from VP to produce an estimate of concentrations of classified groups, associated with CTD data, in vertical bins along the VPR tow-yo path.


# Summary of Processing Steps

The processing of VPR data in R can be broken down into 4 steps. 

1. Image Copying - Organizing VPR images into appropriate folders
2. Manual Classification - A manual check of Visual Plankton's classifications which allows expert corrections
3. Processing - Joining and organization of data outputs from the VPR (ctd .dat files) and VP (roi id and roi measurement text files), computation of average abundance and CTD metrics in vertical bins along the VPR path, and export of data frames which are quality controlled and can then be analyzed and visualized.
4. Plotting - Visualization of processed data frames using ggplot2

Image copying brings in text files from VP describing ROI images file paths (aid files) and uses them to create organized folders of the images contained within each classification group.  
Manual Classification allows a user to sort through the organized image folders and output updated text files describing ROI image file paths (aid files) and sizes (aidmeas files), based on expert classification. 
Processing takes (updated) VP text files describing ROI image file path (aid) and size (aidmeas), and combines this data with VPR CTD text files based on time. It then outputs data in export friendly formats (csv, RData).
Plotting uses the processed data formats exported by Processing and produces visualizations.


Before beginning processing, it is recommended to create a VPR processing environment which will contain commonly used variables and file paths. The simplest and most reproducible way to achieve this is to write an R script where all your cruise and system specific variables are contained, then save the environment as an RData file to be loaded in at the start of any processing scripts. 

```
#set VPR processing environment

#SET WORKING DIRECTORY
wd <- "C:/VPR_PROJECT/COR2019002/SCRIPTS"
setwd(wd)

#CRUISE
cruise <- 'COR2019002'

year <- 2019

#CSV FILE WITH STATION NAMES AND CORRESPONDING DAY/HOUR INFO
station_names_file <- paste0("station_names_", cruise, ".csv")
#example: 'C:/VPR_PROJECT/vp_info/station_names_COR2019002.csv'
# note columns should be labeled : station, day, hour

#DIRECTORY FOR FULL CTD CAST DATA
castdir <- paste0('D:/', cruise, "/", cruise, "_autodeck/")
#example: 'D:/COR2019002/COR2019002_autodeck/'


#AUTOID FOLDER FOR MEASUREMENT DATA
drive <- 'C:/'
auto_id_folder <- paste0(drive, "cruise_", cruise, "/", autoid)
#example: 'E:/COR2019002/autoid' #!!NO BACKSLASH AT END OF STRING

#Path to autoids for each taxa
auto_id_path <- list.files(paste0(auto_id_folder, "/"), full.names = T) #caution if there are autoid versions

#create standard directory for saved data files per cruise
savedir <- paste0(cruise, '_data_files')
dir.create(savedir, showWarnings = FALSE)

#create standard directory for saved data products per cruise and station
stdir <- paste('data_product/', cruise, sep = "")
dir.create(stdir, showWarnings = FALSE, recursive = TRUE)

#DEPTH BIN SIZE FOR AVERAGING
binSize <- 3

####SAVE####
#SAVE ALL FILE PATHS AND SETTINGS AS PROJECT ENVIRONMENT

save.image(file = paste0(cruise,'_env.RData'))
```

Once this environment is set, it can be loaded into any processing session by using
```
load('COR2019002_env.RData') # where COR2019002 is cruise name
```
These variable names will be kept consistent throughout this document for simplicity.

If sharing your processing code with colleagues on version control, keeping your environment variables separate (outside of your git project) will allow you to collaborate while avoiding inconsistencies in file paths or folder names. 


# 1. Image Copying

The first step in processing VPR data is to create a new directory of images organized in folders based on the day and hour of data collection as well as ROI classification, as decided by VP. 

The images are organized by Autodeck by day and hour, but reorganizing them based on classification allows easier human interaction with the data, allowing visual inspection of classifications by VP. Moreover, this directory structure is used by the next step of processing in order to confirm Visual Plankton's classifications. Details regarding directory structure is described in detail in a later section.


To properly organize the images for a given cruise, use the function `vprr::autoid_copy()`. 

```
# create variables
# ---------------------
basepath <- "C:\\data\\cruise_COR2019002\\autoid\\" 
# note this is the same as your auto_id_folder environment variable excpet the file separator is different, 
# because this script will run source code in command line which does not recognize '/' as a file separator

day <- "123"
hour <- "01" # note leading zero is kept
classifier_type <- "svm"
classifier_name <- "myclassifier"

# run file organizer
# ---------------------
autoid_copy(basepath, day, hour, classifer_type, classifier_name)
```

Once image files are organized you can easily look through classification groups manually (in file explorer) to get a sense of which groups are well classified by Visual Plankton and which could benefit from some additional sorting - Step 2.

# 2. Manual Classification

This step is optional although it is recommended by folks at BIO have processed this data. 

The goal of this step is to improve upon the automatic classifications produced by Visual Plankton (VP). The supervised machine learning in VP is a great tool for sorting VPR images but cannot capture some of the finer resolution taxa information desired by oceanographers concerned with plankton community dynamics. For example, the VP software is great at distinguishing marine snow from copepods but not good at distinguishing between species of copepods. The level of detail desired for most projects is in line with an expert taxonomic analysis, so further sorting is required after VP. 

There are a series of functions written to help achieve this goal. VPR images are displayed on the screen one at a time for manual classification, if VP has misclassified an image or if it can be recognized as falling into a finer resolution category the image can be reclassified. At the end of this process, text files, which match the formats produced by VP are reproduced with new classifications. 

The reclassification process is recommended to be run for only specific taxa which VP has clearly struggled to classify based on manual inspection of image folders. The groups chosen will also depend on the groups of interest to researchers. In analysis performed by BIO, Calanus and krill were the main species of interest. VP frequently misclassified krill images in the chaetognath and ctenophore categories, so reclassification experts went through the calanus, krill, chaetognath and ctenophore image folders. It is ideal to not have to reclassify all image folders to improve efficiency, at the very least, we would recommend not sorting through marine snow or blurry image folders, as VP does a good job of classifying these images and the folders can be extremely large and time consuming to sort through. 

Reclassification can also be an opportunity to create new, fine resolution classification groups which could not previously be identified by Visual Plankton. For example, if Visual Plankton identifies a classification group 'copepods' but it would be helpful to split this group into small and large copepods for analysis, small and large copepod classification groups can be added during reclassification. By adding new classification groups to your 'taxa of interest' (`taxa_of_interest` variable in example below) and using the function `vprr::add_new_taxa` *** not a compltete sentence. This allows the expert performing reclassification to label images with these new classification groups. In the example below, Metridia and Paraeuchaeta were added as new classification groups that allowed finer resolution zooplankton community data. After reclassification is run, new files are created for new taxa which are identical in format to original files produced by VP. This feature has become very useful for getting finer scale taxa data. It can also be useful if, during expert reclassification, a certain classification group which was not included in VP classifications, is noticed.


There are three steps of reclassification. 

## Manual Classification: Step A 
This includes prepping the environment by setting some variables:

- Load the processing environment, which includes the `auto_id_folder` variable.
- Set day and hour of interest. 
- Set taxa of interest. These taxa are the existing VP classification groups which require manual classification, as well as any new categories. The `vprr::add_new_taxa()` function sets up the folder structure for any new taxa which have been added to your list of interest. 
- Run manual classification with `vprr::clf_check()`. It has a few optional arguments to customize the reclassification experience, notably `gr` which is a logical value determining whether or not reclassification options appear as pop ups or in the command line, as well as `img_bright`, a logical which determines whether or not the original image is appended with an extra bright version of the image. Having a bright version of the image allows experts to see the outline of the organism better, any thin appendages become more clear and gelatinous organisms like chaetognaths or ctenophores are easier to distinguish.  


```
#### STEP 2- A : CLASSIFICATION CHECK
# -------------------------------------

# Once classified images are sorted by taxa
# ensure classification accuracy by manually 
# looking through classified images

#### USER INPUT REQUIRED ####     

load('COR2019002_env.RData')

day <- '235' 
hr <- '19' # keep leading zeros, must be two characters


taxa_of_interest <-
  c(
    'krill',
    'Calanus',
    'chaetognaths',
    'ctenophores',
    'Other',
    'larval_fish',
    'marine_snow',
    'small_copepod',
    'other_copepods',
    'larval_crab',
    'amphipod',
    'Metridia',
    'Paraeuchaeta',
    'cnidarians'
    
  )


# add new taxa (optional)

add_new_taxa(taxa = taxa_of_interest, auto_id_folder)
# ensures there is proper folder structure for all taxa of interest

# reclassify images
clf_check(day = day, hour= hr, basepath = auto_id_folder,gr = FALSE, 
          taxa_of_interest = taxa_of_interest, scale = 'x300',
          opticalSetting = 'S3')
```

## Step B 
`vprr::clf_check()` produces two file types as a record of reclassification, this step takes those output files and uses them to appropriately modify the original VP files. These output files are found in your working directory, organized inside folders named by the day and hour that the data was collected. The function `vprr::new_aids()` takes these reclassification files and outputs VP format classification files, into your working directory, inside folders named by classification group or taxa.

This step should be run after each hour of data is completed although it can be run at any point following the reclassification of an hour of data.

```

#### STEP 2 - B: REORGANIZE ROIS 
# -----------------------------------------

day_hour_files <-  paste0('d', day, '.h', hr)

misclassified <- list.files(day_hour_files, pattern = 'misclassified_', full.names = TRUE)

reclassify <- list.files(day_hour_files, pattern = 'reclassify_', full.names = TRUE)


# MOVE ROIS THAT WERE MISCLASSIFIED INTO CORRECT FILES
new_aids(reclassify, misclassified, auto_id_folder)


```

## Step C

The last step of reclassification includes some manual file organization and final checks. Once all classification (aid & aidmeas) files have been created (with `vprr::new_aids`), they are contained in taxa folders inside your working directory. These files should be reorganized in a new directory which will become the new auto_id_folder. Remember that auto id files from taxa which were not reclassified should be added to the new auto_id_folder. 


Once files have been manually organized, there are two checks that can be performed. `vprr::aid_file_check()` is strongly recommended, it removes any empty aid files, created if there are no images of a specific classification group in that hour of data, which can cause errors in processing down the line. It also checks that aid and aidmeas files are matching within an hour of data, that they include the same number of ROIs, and that the VPR tow number for all files is the same. 

There have been bugs in the past where aid and aidmeas files within an hour were different lengths, because these files represent the same data but the only key between them is line number, it is important that they contain the exact same number of data points. There was also a bug in the past where some images from different VPR tows were getting pulled together due to a formatting error in raw data on IML2018051. 

Part two of these manual checks is a 'krill check'. This process was developed when 2018 data was processed at BIO. It was quickly realized that there would be small amounts of human error in the reclassification process, due to the small number of images in the krill category, even one mistaken image was creating a large difference in processed data. A check was developed to pull all krill images and allow for manual reorganization of them within file explorer, to remove any images which were accidentally classified as krill. This check could be performed with any classification group where there are a small number of images and accuracy is of high importance. 

Krill check is a little more involved due to it's manual nature and is described below.

```

#ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT ALERT #
#==========================================================================================#
#                            ONCE ALL AID FILES ARE COMPLETE                               #
#==========================================================================================#
#                                     REQUIRED!!!!                                         #
#       manual organization of new aid and aidmeas files into base path directory          #
#==========================================================================================#


#### STEP 2 - C: CHECK FILES
# --------------------------------


# aid check step
# removes empty aid files, and checks for errors in writing

aid_file_check(basepath, cruise) #OUTPUT: text log 'CRUISE_aid_file_check.txt in working directory


# optional -----------------------------------------------------------------------------------------------------------------------------------
# manual krill check can be done to verfiy krill images. 
# Sometimes this is required because of the low volume of krill, means high sensitivity to error.


```

### Krill Check

Krill check requires 3 parts. First, images must be pulled from reclassification into new folders which can be visually checked. Second, images should be manually sorted in file explorer. This requires deleting any non - krill images from the folders created in the first part of krill check. Thirdly, the manually organized folders must be compared with aid and aidmeas files (`image_check`) and any ROI strings which are no longer present in folders should be removed from text files (aid and aidmeas). 

```

image_copy(auto_id_folder, taxas.of.interest, day, hour)

# perform manual removal of images not matching classification group

image_check_folder <- file.path(auto_id_folder, taxas.of.interest, fsep = '\')

image_check(image_check_folder)

```

Note, that in the case where data set is very small or requires few reclassification adjustments, the krill check procedure could be used as the only reclassification process. Cons to using this method include extensive time commitment, potential for manual error, and lack of record keeping. It can quickly become difficult to keep track of aid versions, so ensure you have a consistent and documented directory naming scheme/ archiving system. 



# 3. Processing

This is the main chunk of coding required to produce useable data products from the VPR. This step can be done before reclassification to get an initial idea of the data and see if reclassification is required. It should be done after reclassification to produce data products which can be plotted, analyzed, and shared. 

The following is a walk through of processing data from 2018

First, all libraries should be loaded and the processing environment, described in Summary of Processing Steps should be loaded. 
```
##### STEP 3: PROCESSING 
# ==================================================================================================================================================
# PROCESSING OF VPR DATA

library(vprr)

#### FILE PATHS & SETTINGS --------------------------------------------------------------------------------------------------------------------
# loads processing environment specific to user

load('IML2018051_env.RData')

```


This section allows a user to process all stations of a particular cruise in a loop, this can be modified or removed based on personal preference

```
##### STATION LOOP ----------------------------------------------------------------------------------------------------------------------------

all_stations <- read.csv(station_names_file, stringsAsFactors = FALSE) 
all_stations_of_interest <- unique(all_stations$station) 


for (j in 1:length(all_stations_of_interest)){
  
  station_of_interest <- all_stations_of_interest[j] 
  
  cat('Station', station_of_interest, 'processing... \n')
  cat('\n')
  ```
 
 Optical settings and image volume variables should be set, if they are consistent throughout the cruise, they could also be added to the processing environment.
 
 
  
  ```
  #==========================================#
  #   Set optical settings  & Image Volume   #
  #   !Should be updated with each cruise!   #
  #==========================================#
  if(cruise == "IML2018051") {
    
    #VPR OPTICAL SETTING (S0, S1, S2 OR S3)
    opticalSetting <- "S2"
    imageVolume <- 108155 #mm^3
  }
  ```
  CTD casts are loaded in using `vprr::list_ctd_files` to find files and `vprr::read_ctd_vpr` to read in files.
  
  ```
  #get day and hour info from station names list
  dayhour <- get_dayhour(station_of_interest, file = station_names_file)
  
  
  ##### PULL CTD CASTS ----------------------------------------------------------------------------------------------------------------------------
  # get file path for ctd data
  
  # list ctd files for desired day.hours
  ctd_files <- list_ctd_files(castdir, cruise, dayhour) 
  
  ##### READ CTD DATA ----------------------------------------------------------------------------------------------------------------------------
  
  ctd_dat_combine <- read_ctd_vpr(ctd_files, station_of_interest)
  
  cat('CTD data read complete! \n')
  cat('\n')
  
  ```
  VPR data files are then found, this coding is tailored to the Visual Plankton directory structure and could be modified if another directory structure was preferred. However, if a new directory structure is used please proceed with extreme caution as function internals may also rely on VP structure and be difficult to update.
  
  ```
  ##### FIND VPR DATA FILES ----------------------------------------------------------------------------------------------------------------------
  
  # Path to aid for each taxa                               
  aid_path <- paste0(auto_id_path, '/aid/')              
  # Path to mea for each taxa                               
  aidmea_path <- paste0(auto_id_path, '/aidmea/')        
  
  # AUTO ID FILES
  aid_file_list <- list()
  aidmea_file_list <- list()
  for (i in 1:length(dayhour)) {
    aid_file_list[[i]] <-
      list.files(aid_path, pattern = dayhour[[i]], full.names = TRUE)
    # SIZE DATA FILES
    aidmea_file_list[[i]] <-
      list.files(aidmea_path, pattern = dayhour[[i]], full.names = TRUE)
  }
  
  aid_file_list_all <- unlist(aid_file_list)
  aidmea_file_list_all <- unlist(aidmea_file_list)
  
  remove(aid_file_list, aidmea_file_list, aid_path, aidmea_path)
  
  ```
  
  ROI and measurement data files are then read using `vprr::read_aid`.
  
  
  ```
  ##### READ ROI AND MEASUREMENT DATA ------------------------------------------------------------------------------------------------------------
  
  
  # ROIs
  roi_dat_combine <-
    read_aid(
      file_list_aid = aid_file_list_all,
      file_list_aidmeas = aidmea_file_list_all,
      export = 'aid',
      station_of_interest = station_of_interest,
      opticalSetting = opticalSetting
    )
  
  # MEASUREMENTS
  roimeas_dat_combine <-
    read_aid(
      file_list_aid = aid_file_list_all,
      file_list_aidmeas = aidmea_file_list_all,
      export = 'aidmeas',
      station_of_interest = station_of_interest,
      opticalSetting = opticalSetting
    )
  
  cat('ROI and measurement data read in complete! \n')
  cat('\n')
  ```
  
  This section should be tailored to individual processing needs, it is common that specific stations will have unique data issues that need small, manual corrections. This example shows an issue in the 2018 data where time stamps jumped in the raw data causing a gap between casts.
  
  
  ```
  ##### FIX UNIQUE STATION ISSUES -----------------------------------------------------------------------------------------------------------------
  
    
    ##Correct time stamps from CAP 3.1 station, if used <- needs re-evaluation
    
    if("CAP 3.1" %in% station_of_interest) {
      
      ctd_dat_combine <- ctd_dat_combine %>%
        dplyr::mutate(., time_ms = ifelse(time_ms < 25000000, time_ms + 86400000, time_ms))
      
      roi_dat <- roi_dat %>%
        dplyr::mutate(., time_ms = ifelse(time_ms < 25000000, time_ms + 86400000, time_ms))
      
      roimeas_dat_combine <- roimeas_dat_combine %>%
        dplyr::mutate(., time_ms = ifelse(time_ms < 25000000, time_ms + 86400000, time_ms))
      
      cat('CAP 3.1 data corrected! \n')
      cat('\n')
    }
    
  
  ```
  
  Next, CTD and ROI data is merged to create a data frame describing both water properties and classified images. The function used is `vprr::merge_ctd_roi`.
  
  
  ```
  ##### MERGE CTD AND ROI DATA ---------------------------------------------------------------------------------------------------------------------
  ctd_roi_merge <- merge_ctd_roi(ctd_dat_combine, roi_dat_combine)
  
  cat('CTD and ROI data combined! \n')
  cat('\n')
  
  ```
  Before final export of data products, a few calculated variables are added, `avg_hr` is a variable representing time in hours rather than milliseconds, which simplifies plotting and is more easily understood. `sigmaT` is also calculated using`oce::swSigmaT` to give a standardized density variable. `add_ymd` is used to add a time stamp with POSIXct signature to the data frame in Y-M-D h:m:s format, this is good for completeing metadata as well as referencing VPR data to oher sampling methods. This step also caluclates depth from pressure using `oce::swDepth`. 
  
  A note on time stamps: There are often issues with time offsets in VPR data. The day/hour information used throughout this processing will be in the timezone of the computer used to process images in autoDeck. It is ideal to set this computer's time to UTC to avoid any confusion. 
  
  ```
  ##### CALCULATED VARS ----------------------------------------------------------------------------------------------------------------------------
  
  # add avg hr and sigma T data and depth
 data <- ctd_roi_merge %>%
    dplyr::mutate(., avg_hr = time_ms / 3.6e+06) %>%
    dplyr::mutate(
      .,
      sigmaT = swSigmaT(
        ctd_roi_merge$salinity,
        ctd_roi_merge$temperature,
        ctd_roi_merge$pressure
      )) %>%
    dplyr::mutate(., depth = oce::swDepth(ctd_roi_merge$pressure)) # note that default latitude is used (45)
    
    
    data <- add_ymd(data, year)
  
  cat('Initial processing complete! \n')
  cat('\n')
  
  # clean environment
  remove(ctd_roi_merge)
  ```
  VPR and CTD data is then binned by depth which allows for better interpretation and plotting. This is done by creating an `oce` CTD object using `vprr::create_oce_vpr`, then binning data using `vprr::bin_vpr_data`. Concentrations are calculated for each classification group. 
  
  ```
  
  ##### BIN DATA AND DERIVE CONCENTRATION ----------------------------------------------------------------------------------------------------------
  
  ctd_roi_oce <- create_oce_vpr(data)
  
  # bin data
  vpr_depth_bin <- bin_vpr_data(ctd_roi_oce = ctd_roi_oce, binSize =  binSize, imageVolume = imageVolume)
  
  # get list of valid taxa
  taxas_list <- unique(roimeas_dat_combine$taxa)
  
  # calculate concentrations for each taxa
  taxa_conc_n <- calculate_vpr_concentrations(data, taxas_list, station_of_interest, binSize, imageVolume)
  
  cat('Station', station_of_interest, 'processing complete! \n')
  cat('\n')
  ```
  Finally, data is saved as RData and csv files for export and plotting. Data is also save as an `oce` object in order to preserve both data and metadata in a efficient format.
  
  ```
  ##### SAVE DATA ---------------------------------------------------------------------------------------------------------------------------------
  # Save oce object
  oce_dat <- save_vpr_oce(taxa_conc_n)
  save(file = paste0(savedir, '/oceData_', station_of_interest,'.RData'), oce_dat)
  
  
  # Save RData files
  save(file = paste0(savedir, '/ctdData_', station_of_interest,'.RData'), ctd_dat_combine)
  save(file = paste0(savedir, '/stationData_', station_of_interest,'.RData'), data)
  save(file = paste0(savedir, '/meas_dat_', station_of_interest,'.RData'), roimeas_dat_combine)
  save(file = paste0(savedir, '/bin_dat_', station_of_interest,'.RData'), vpr_depth_bin)
  
  cat('CTD, ROI-VPR merge, ROI measurement saved as RData! \n')
  cat('\n')
  
  # Write csv files
  # write.csv(file = paste0(stdir, '/vpr_data_unbinned', station_of_interest, '.csv'), data, row.names = F)
  # write.csv(file = paste0(stdir, '/vpr_meas', station_of_interest, '.csv'), roimeas_dat_combine)
  write.csv(file = paste0(stdir, '/vpr_data_binned', station_of_interest, '.csv'), taxa_conc_n)
  
  cat('ROI measurments, ROI-CTD merge-unbinned, and ROI-CTD merge-binned written to csv! \n')
  cat('\n')
  
} #end of station loop
```

# 4. Plotting


The following are a few example plots. The first step to plotting is properly loading in the processed VPR data objects developed in processing. The environment, described in the Summary of Processing Steps section should also be loaded. The individual data files are found by distinct names eg "stationData", note that your directory structure may be different depending on your `savedir` where data files were saved during processing. 

Note that the following plotting examples are best used for tow-yo pattern VPR deployments. 



```

##### FILE PATH & SETTINGS -----------------------------------------------------------------------------------------------------------------------

library(vprr)

# loads all file paths and environment vars specific to User
load('COR2019002_env.RData')

#find all data files
fn_all_st <- list.files(paste0(cruise, "_data_files/"), pattern = "stationData", full.names = T)
fn_all_meas <- list.files(paste0(cruise, "_data_files/"), pattern = "meas", full.names = T)
fn_all_conc <- list.files(paste0("data_product/", cruise, "/"), pattern = "data_binned", full.names = T)
fn_all_bin <- list.files(paste0(cruise,"_data_files/"), pattern = 'bin_dat', full.names = T)
```


Once files are loaded it is often easiest to run a loop of plots for all stations in a cruise, in order to get comparable plots for analysis. The example below shows a loop being initiated which runs through a list of stations described by a csv file. This loop also isolates two specific classification groups to plot (Calanus and krill), which allows a better focus in plotting. 

```

####START STATION LOOP ---------------------------------------------------------------------------------------------------------------------------

setwd(wd)

all_stations <- read.csv(station_names_file, stringsAsFactors = FALSE)
all_stations_of_interest <- unique(all_stations$station)

taxa_to_plot <- c("Calanus", "krill")


for (j in 1:length(all_stations_of_interest)){
  
  setwd(wd)
  station <- all_stations_of_interest[j]
  
  cat('station', station ,'starting to plot.... \n')
  cat('\n')
  ```
  
Next, data files are loaded for the specific station being looped. This loads in all relevant RData files as well as the concentration data saved as a csv file
  
  ```
  #load station roi and ctd data
  fn_st <- grep(fn_all_st, pattern = station, value = TRUE, ignore.case = TRUE)
  fn_meas <- grep(fn_all_meas, pattern = station, value = TRUE, ignore.case = TRUE)
  fn_conc <- grep(fn_all_conc, pattern = station, value = TRUE, ignore.case = TRUE)
  fn_bin <- grep(fn_all_bin, pattern = station, value = TRUE, ignore.case = TRUE)
  
  load(fn_st)
  load(fn_meas)
  load(fn_conc)
  load(fn_bin)
  
  
 # load concentration data
  taxa_conc_n <- read.csv(fn_conc, stringsAsFactors = F)
  
  station_name <- paste('Station ', station)
  ```
  
The last bit of set up required, sets the directory in which plots will be saved and some generic plot size arguments which will control how large the saved .png files are. 
  
  ```
  # directory for plots
  stdir <- paste0('figures/', cruise, '/station', station)
  dir.create(stdir, showWarnings = FALSE, recursive = TRUE)
  setwd(stdir)
  
  width = 1200
  height = 1000
  ```
  
  This first plot follows a consistent format for creating VPR contour plots with variable contours behind VPR concentration data over the tow path. The main function used is `vprr::conPlot_EC` which uses a standard VPR data frame (produced from Processing) and plots the background contours. Interpolation methods can be adjusted based on data or preference. Then the VPR tow path can be added on top of contours, this displays the concentration data as bubbles. This method can be repeated with various water parameters used to calculate the contours, by changing the `var` argument in `vprr::conPlot_EC`.  
  
  
  ```
  # Density (sigmaT)
  png('conPlot_taxa_dens.png', width = width, height = height)
  p <- conPlot_EC(taxa_conc_n[taxa_conc_n$taxa %in% c(taxa_to_plot),], var = 'density', dup = 'strip', method = 'oce', bw = 0.5)
  p <- p + geom_line(data = data, aes(x = avg_hr - min(avg_hr), y = pressure), col = 'snow4', inherit.aes = FALSE) +
    geom_point(data = taxa_conc_n[taxa_conc_n$taxa %in% c(taxa_to_plot),], aes(x = avg_hr, y = min_pressure, size = conc_m3), alpha = 0.5)+
    ggtitle(station_name ) +
    labs(size = expression("Concentration /m" ^3), fill = 'Density')+
    scale_size_continuous(range = c(0, 10)) +
    facet_wrap(~taxa, ncol = 1, scales = 'free') +
    theme(legend.key.size = unit(0.8, 'cm'),
          axis.title = element_text(size = 20),
          strip.text = element_text(size = 20),
          plot.title = element_text(size = 32), 
          axis.ticks = element_line(size = 1, lineend = 'square'),
          axis.text = element_text(size = 30),
          legend.text = element_text(size = 20),
          legend.title = element_text(size = 25)
    )
  
  print(p)
  dev.off()
  ```
  Another common plot which allows great data exploration of vertical distributions is the profile plot created by `vprr::profile_plot`, which will shows both water properties and the concentrations of specific classification groups. Temperature, salinity, fluorescence and density plots are created with custom axes. Concentration plots show all binned data compressed into a profile format (without the time axis), as well as an average line which calculates the mean concentration of all bins at a particular depth. The average line can be great to identify vertical trends in data as well as compare to a depth integrated method such as a vertical net tow. 
  
  
  ```
  
    png('profilePlots_RK.png', width = 1000, height = 500)
    p <- profile_plot(taxa_conc_n, taxa_to_plot)
    print(p)
    dev.off()
    ```
    
TS plots can also be helpful to visualize concentration data in comparison with water masses. Below, we create a TS plot in ggplot (with labeled isopycnals), and overlay concentration bubbles for each selected classification group. Using `vprr::plotTS_balloon` streamlines isopycnal creation and labelling, other variables can also be selected for controlling the size of bubbles in TS space. Ggplot2 grammar makes it easy to add to the basic TS balloon plot, for example to facet by classification group or adjust axis labels and sizing. 


```

####TS BALLOON PLOT ----------------------------------------------------------------------------------------------------------------------------
  
  # plot by taxa
  taxa_conc <- taxa_conc_n[taxa_conc_n$conc_m3 > 0,]
  png('TS_conc_taxa.png', width = 1000, height = 500)
  p <- plotTS_balloon(taxa_conc[taxa_conc$taxa %in% c(taxa_to_plot),], var = 'conc_m3') +
    facet_wrap(~taxa, nrow = 1) +
    theme(strip.text = element_text(size = 18),
          axis.title = element_text(size = 20),
          panel.spacing = unit(2, 'lines'))
  print(p)
  dev.off()
  
  
  cat('station', station, 'complete! \n')
  cat('\n')
  
} # end station loop
  ```
  

  
These plotting examples show plots being saved as .png files, this was the method used for 2018 and 2019 data at BIO but it is worth exploring other possible options as well. Using RMarkdown to create plots can be great for producing reports and keeping track of plot versions. Using RMarkdown documents means that the code and plot are always directly associated and there is no concern of mismatch between plot version and code version. Markdown can be a little slow when creating many large VPR plots, but might be best for smaller projects.
  


# Warnings and debugging information

In closing, it is important to note that these functions were created for a specific project here at BIO and have not been tested on varied data structures. If using VPR data collected from profile or moored deployments, or using different image processing and classification software, there may be major differences throughout processing. Hopefully this can give a template of the required processing and an idea of how to initially explore data. 

# Directory Structure

Visual Plankton (MatLab image classification software) requires a very specific directory structure in order to function. Since this processing is meant to directly follow this image classification, it relies on the same directory structure. This allows a smooth transition between the MatLab classifications and the completion of processing in R. The directory structure required is described below


- C:/
  - data
  
    - cruise_name
    
      - autoid
        - taxa
          - aid
          - aidmea
          
      - clpar
      
      - feature
        - vprtow#
        
      - idsize
      
      - rois
        - vprtow#
          - day
            - hour
        
      - tefeature
        - vprtow#
          - day
            - hour
        
      - trrois
        - vprtow#
          - day
            - hour
            
            
            
# Glossary

**Image Processing Toolbox**
-	A MatLab library toolbox required to run VP, if running program at BIO, there is only one license for this toolbox.

**ROI**
-	Region of interest, images identified by autodeck within VPR frames based on settings defined in autoDeck program

**TRROIS**
-	Training set of images used to train machine learning algorithm in Visual Plankton

**VP**
-	Visual Plankton program run in MatLab

**Confusion Matrix**
-	A matrix of actual versus predicted values used to determine accuracy of model results

**Hid, hidmea files**
-	files produced by visual plankton program defining features used to classify taxa. Hid files contain general features, hidmeas files contain size based features

**SVM**
-	Support vector machine, type of machine learning algorithm

**Neural Net**
-	A type of machine learning algorithm 

**CTD**
-	Conductivity, Temperature and depth sensor instrument used in oceanogrpahy

**VPR**
-	Visual Plankton Recorder, oceanographic instrument used to image small volumes of water for the purpose of capturing images of plankton

**Auto Deck**
-	software which pulls plankton images from Visual Plankton Recorder frames based on specific settings

**GUI**
-	An interactive interface used in computer programming

**Aid files**
- Visual Plankton file output text file, listing file path information for ROI's of a spedific classification group

**AidMeas files** 
- Visual Plankton ouptut text file, listing measurement data for ROI's of a specific classification group. Unit is pixels and columns are 'Perimeter', 'Area', 'width1', 'width2', 'width3', 'short_axis_length', 'long_axis_length'

**Day**
- Julian calendar day on which VPR data was collected (three digits)

**Hour**
- Two digit hour (24 hour clock) describing time at which VPR data was collected

**vprtow#**
- A numeric code which is unique to each vpr deployment

**station**
- A named geographic location, where the VPR was deployed

**Classification group or Taxa**
- A defined group under which VPR images can be classified, often represents a taxonomic group (e.g. Krill), but can also de defined by image type (e.g. 'bad_image_blurry'), or other (e.g. 'marine_snow'), should be one continuous string (no spaces)

**BIO**
- Bedford Institute of Oceanography, a research institute in Halifax NS, Canada, where VPR research is being done.

**Tow-yo**
- A VPR deployment method where the VPR is towed behind a vessel while being raised and lowered through the water column in order to sample over both depth and distance.

**Optical Setting**
- A VPR setting controlling image magnification and field of view, which can be S0, S1, S2 or S3, where S0 has the greatest magnification and smallest image volume, and S3 has the least magnification and largest image volume.

**Image volume**
- The measured volume of water captured within a VPR image. Calculated based on optical setting and VPR standards...?

**Auto ID**
- The automatic classification given to an image from Visual Plankton machine learning algorithm


